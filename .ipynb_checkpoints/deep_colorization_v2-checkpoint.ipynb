{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, threading\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input\n",
    "from keras.layers.core import RepeatVector, Permute\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv2D, UpSampling2D, InputLayer, Conv2DTranspose, Input, Reshape, merge, concatenate\n",
    "from keras.initializers import TruncatedNormal\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from skimage.color import rgb2lab, lab2rgb, rgb2gray, gray2rgb\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imsave\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "\n",
    "def batch_apply(ndarray, func, *args, **kwargs):\n",
    "    batch = []\n",
    "    for sample in ndarray:\n",
    "        batch.append(func(sample, *args, **kwargs))\n",
    "    return np.array(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "inception = InceptionResNetV2(weights='imagenet', include_top=True)\n",
    "inception.graph = tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inception_embedding(grayscaled_rgb):\n",
    "    with inception.graph.as_default():\n",
    "        embed = inception.predict(grayscaled_rgb)\n",
    "    return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    #Inputs\n",
    "    embed_input = Input(shape=(1000,))\n",
    "    encoder_input = Input(shape=(256, 256, 1,))\n",
    "    \n",
    "    #Encoder\n",
    "    encoder_output = Conv2D(64, (3,3), activation='relu', padding='same', strides=2,\n",
    "                            bias_initializer=TruncatedNormal(mean=0.0, stddev=0.05))(encoder_input)\n",
    "    encoder_output = Conv2D(128, (3,3), activation='relu', padding='same',\n",
    "                            bias_initializer=TruncatedNormal(mean=0.0, stddev=0.05))(encoder_output)\n",
    "    encoder_output = Conv2D(128, (3,3), activation='relu', padding='same', strides=2,\n",
    "                            bias_initializer=TruncatedNormal(mean=0.0, stddev=0.05))(encoder_output)\n",
    "    encoder_output = Conv2D(256, (3,3), activation='relu', padding='same',\n",
    "                            bias_initializer=TruncatedNormal(mean=0.0, stddev=0.05))(encoder_output)\n",
    "    encoder_output = Conv2D(256, (3,3), activation='relu', padding='same', strides=2,\n",
    "                            bias_initializer=TruncatedNormal(mean=0.0, stddev=0.05))(encoder_output)\n",
    "    encoder_output = Conv2D(512, (3,3), activation='relu', padding='same',\n",
    "                            bias_initializer=TruncatedNormal(mean=0.0, stddev=0.05))(encoder_output)\n",
    "    encoder_output = Conv2D(512, (3,3), activation='relu', padding='same',\n",
    "                            bias_initializer=TruncatedNormal(mean=0.0, stddev=0.05))(encoder_output)\n",
    "    encoder_output = Conv2D(256, (3,3), activation='relu', padding='same',\n",
    "                            bias_initializer=TruncatedNormal(mean=0.0, stddev=0.05))(encoder_output)\n",
    "    \n",
    "    #Fusion\n",
    "    fusion_output = RepeatVector(32 * 32)(embed_input) \n",
    "    fusion_output = Reshape(([32, 32, 1000]))(fusion_output)\n",
    "    fusion_output = concatenate([encoder_output, fusion_output], axis=3) \n",
    "    fusion_output = Conv2D(256, (1, 1), activation='relu', padding='same',\n",
    "                            bias_initializer=TruncatedNormal(mean=0.0, stddev=0.05))(fusion_output)\n",
    "    \n",
    "    #Decoder\n",
    "    decoder_output = Conv2D(128, (3,3), activation='relu', padding='same',\n",
    "                            bias_initializer=TruncatedNormal(mean=0.0, stddev=0.05))(fusion_output)\n",
    "    decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
    "    decoder_output = Conv2D(64, (3,3), activation='relu', padding='same',\n",
    "                            bias_initializer=TruncatedNormal(mean=0.0, stddev=0.05))(decoder_output)\n",
    "    decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
    "    decoder_output = Conv2D(32, (3,3), activation='relu', padding='same',\n",
    "                            bias_initializer=TruncatedNormal(mean=0.0, stddev=0.05))(decoder_output)\n",
    "    decoder_output = Conv2D(16, (3,3), activation='relu', padding='same',\n",
    "                            bias_initializer=TruncatedNormal(mean=0.0, stddev=0.05))(decoder_output)\n",
    "    decoder_output = Conv2D(2, (3, 3), activation='tanh', padding='same',\n",
    "                            bias_initializer=TruncatedNormal(mean=0.0, stddev=0.05))(decoder_output)\n",
    "    decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
    "    \n",
    "    model = Model(inputs=[encoder_input, embed_input], outputs=decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(shear_range=0.2, zoom_range=0.2, rotation_range=20, horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images(rgb, input_size=(256, 256, 3), embed_size=(299, 299, 3)):\n",
    "    # Resize for embed and Convert to grayscale\n",
    "    gray = gray2rgb(rgb2gray(rgb))\n",
    "    gray = batch_apply(gray, resize, embed_size, mode='constant')\n",
    "    # Zero-Center [-1, 1]\n",
    "    gray = gray * 2 - 1\n",
    "    # Generate embeddings\n",
    "    embed = create_inception_embedding(gray)\n",
    "    \n",
    "    # Resize to input size of model\n",
    "    re_batch = batch_apply(rgb, resize, input_size, mode='constant')\n",
    "    # RGB => L*a*b*\n",
    "    re_batch = batch_apply(re_batch, rgb2lab)\n",
    "    \n",
    "    # Extract L* into X, zero-center and normalize\n",
    "    X_batch = re_batch[:,:,:,0]\n",
    "    X_batch = X_batch/50 - 1\n",
    "    X_batch = X_batch.reshape(X_batch.shape+(1,))\n",
    "    \n",
    "    # Extract a*b* into Y and normalize. Already zero-centered.\n",
    "    Y_batch = re_batch[:,:,:,1:]\n",
    "    Y_batch = Y_batch/128\n",
    "    \n",
    "    return [X_batch, embed], Y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_a_b_gen(images, batch_size):\n",
    "    while True:\n",
    "        for batch in datagen.flow(images, batch_size=batch_size):\n",
    "            yield process_images(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = '../tiny-imagenet-200/'\n",
    "\n",
    "training_files, testing_files = train_test_split(shuffle(os.listdir(DATASET)), test_size=0.15)\n",
    "# training_files = ['test_2.JPEG']\n",
    "# testing_files = ['test_2.JPEG']\n",
    "\n",
    "def getImages(DATASET, filelist, transform_size=(299, 299, 3)):\n",
    "    \"\"\"Reads JPEG filelist from DATASET and returns float represtation of RGB [0.0, 1.0]\"\"\"\n",
    "    img_list = []\n",
    "    for i,filename in enumerate(filelist):\n",
    "        if i < 100: \n",
    "            if 'JPEG' in filename:\n",
    "                image_in = img_to_array(load_img(DATASET + filename))\n",
    "#                 image_in = img_to_array(load_img(filename))\n",
    "\n",
    "                image_in = image_in/255\n",
    "\n",
    "                if transform_size is not None:\n",
    "                    image_in = resize(image_in, transform_size, mode='reflect')\n",
    "\n",
    "                img_list.append(image_in)\n",
    "    img_list = np.array(img_list)\n",
    "    \n",
    "    return img_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=RMSprop(lr=1e-3), loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, training_files, batch_size=1, epochs=30, steps_per_epoch=1):\n",
    "    training_set = getImages(DATASET, training_files)\n",
    "    train_size = int(len(training_set)*0.85)\n",
    "#     train_size = int(len(training_set)*1)\n",
    "    train_images = training_set[:train_size]\n",
    "    val_images = training_set[train_size:]\n",
    "    val_steps = (len(val_images)//batch_size)\n",
    "    print(\"Training samples:\", train_size, \"Validation samples:\", len(val_images))\n",
    "\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=15, verbose=1, min_delta=1e-5),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, cooldown=0, verbose=1, min_lr=1e-8),\n",
    "        ModelCheckpoint(monitor='val_loss', filepath='model_output/colorize.hdf5', verbose=1,\n",
    "                         save_best_only=True, save_weights_only=True, mode='auto'),\n",
    "        TensorBoard(log_dir='./logs', histogram_freq=10, batch_size=20, write_graph=True, write_grads=True,\n",
    "                    write_images=False, embeddings_freq=0)\n",
    "    ]\n",
    "\n",
    "#     model.fit_generator(image_a_b_gen(train_images, batch_size), epochs=epochs, steps_per_epoch=steps_per_epoch,\n",
    "#                         verbose=1)\n",
    "    \n",
    "    model.fit_generator(image_a_b_gen(train_images, batch_size), epochs=epochs, steps_per_epoch=steps_per_epoch,\n",
    "                        verbose=1, callbacks=callbacks, validation_data=process_images(val_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, testing_files, save_actual=False, save_gray=False):\n",
    "    test_images = getImages(DATASET, testing_files)\n",
    "    model.load_weights(filepath='model_output/colorize.hdf5', by_name=True)\n",
    "\n",
    "    print('Preprocessing Images')\n",
    "    X_test, Y_test = process_images(test_images)\n",
    "    \n",
    "    print('Predicting')\n",
    "    # Test model\n",
    "    output = model.predict(X_test)\n",
    "    \n",
    "    # Rescale a*b* back. [-1.0, 1.0] => [-128.0, 128.0]\n",
    "    output = output * 128\n",
    "    Y_test = Y_test * 128\n",
    "\n",
    "    # Output colorizations\n",
    "    for i in range(len(output)):\n",
    "        name = testing_files[i].split(\".\")[0]\n",
    "        print('Saving '+str(i)+\"th image \" + name + \"_*.png\")\n",
    "        \n",
    "        lightness = X_test[0][i][:,:,0]\n",
    "        \n",
    "        #Rescale L* back. [-1.0, 1.0] => [0.0, 100.0]\n",
    "        lightness = (lightness + 1) * 50\n",
    "        \n",
    "        predicted = np.zeros((256, 256, 3))\n",
    "        predicted[:,:,0] = lightness\n",
    "        predicted[:,:,1:] = output[i]\n",
    "        plt.imsave(\"result/predicted/\" + name + \".jpeg\", lab2rgb(predicted))\n",
    "        \n",
    "        if save_gray:\n",
    "            bnw = np.zeros((256, 256, 3))\n",
    "            bnw[:,:,0] = lightness\n",
    "            plt.imsave(\"result/bnw/\" + name + \".jpeg\", lab2rgb(bnw))\n",
    "        \n",
    "        if save_actual:\n",
    "            actual = np.zeros((256, 256, 3))\n",
    "            actual[:,:,0] = lightness\n",
    "            actual[:,:,1:] = Y_test[i]\n",
    "            plt.imsave(\"result/actual/\" + name + \".jpeg\", lab2rgb(actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, training_files, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(model, testing_files, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist = shuffle(os.listdir('result/predicted/'))\n",
    "filelist = filelist[:1]\n",
    "\n",
    "fig, ax = plt.subplots(2, 3, figsize=(16,16))\n",
    "row = 0\n",
    "for filename in filelist:\n",
    "    folder = 'result/bnw/'\n",
    "    image_in = img_to_array(load_img(folder + filename))\n",
    "    image_in = image_in/255\n",
    "    ax[row,0].imshow(image_in)\n",
    "    \n",
    "    folder = 'result/predicted/'\n",
    "    image_in = img_to_array(load_img(folder + filename))\n",
    "    image_in = image_in/255\n",
    "    ax[row,1].imshow(image_in)\n",
    "    \n",
    "    folder = 'result/actual/'\n",
    "    image_in = img_to_array(load_img(folder + filename))\n",
    "    image_in = image_in/255\n",
    "    ax[row,2].imshow(image_in)\n",
    "    \n",
    "    row += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
